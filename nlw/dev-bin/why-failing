#!/usr/bin/perl
$|=1;

use strict;
use warnings;
use Getopt::Long;
use Pod::Usage;
use LWP::Simple qw(get);
use YAML qw();
use File::Path qw(rmtree);
use List::MoreUtils qw(before last_index any);
use File::Slurp qw(slurp);
use Text::ParseWords qw(quotewords);

###############################################################################
# Read in our command line arguments
my ($help, $man, $verbose);
my $stci_url = 'http://galena.socialtext.net/stci';
my $branch   = 'master';
my $testrun  = 'current';
my $testset  = 'socialtext';
my $dryrun   = undef;
my $unittest = undef;
my $count_all_failures = undef;
my @candidates;

GetOptions(
    'stci=s'      => \$stci_url,
    'branch=s'    => \$branch,
    'run=s'       => \$testrun,
    'dryrun'      => \$dryrun,
    'verbose'     => \$verbose,
    'candidate=s' => \@candidates,
    'all'         => \$count_all_failures,
    'help|?'      => \$help,
    'man'         => \$man,
) || pod2usage(1);
pod2usage(1) if ($help);
pod2usage(-exitstatus => 0, -verbose => 2) if ($man);

$unittest = shift @ARGV;
pod2usage(1) unless ($unittest);

$verbose += ( $dryrun ) ? $dryrun : 0;    # --dryrun implies --verbose

###############################################################################
# Make sure that the failing test exists
status("Testing: $unittest");
unless (-e $unittest) {
    die "Can't find '$unittest'; did you 'cdnlw'?\n";
}

###############################################################################
# Build up a set of "candidate tests"; everything that we think ran since the
# last "clean" or "destructive" fixture, leading up to the failing test.
unless (@candidates) {
    # get the YAML file outlining the order in which the tests were run.
    verbose("Fetching test order for $branch/$testrun...");
    my $order_yaml = get("$stci_url/$branch/$testrun/test-order.yaml");
    die "Unable to get test order YAML file.\n" unless $order_yaml;

    my $order     = YAML::Load($order_yaml);
    my @all_tests = @{ $order->{$testset} };

    # everything before the failing test
    @candidates = before { $_ eq $unittest } @all_tests;
    unless (@candidates) {
        die "Couldn't find '$unittest' in the YAML; has that test run yet?\n";
    }

    # remove tests that don't exist; could have leftovers from previous STCI run
    @candidates = grep { -e $_ } @candidates;

    # remove everything that ran before the most recent 'clean' fixture
    my $last_clean = last_index { is_fixture_used_by_test('clean', $_) } @candidates;
    if ($last_clean > -1) {
        @candidates = splice @candidates, $last_clean;
    }

    # remove everything that ran before (and including) the most recent
    # 'destructive' fixture
    my $last_destructive
        = last_index { is_fixture_used_by_test('destructive', $_) } @candidates;
    if (($last_destructive > -1) && ($last_destructive < $#candidates)) {
        @candidates = splice @candidates, $last_destructive + 1;
    }
}

# spit out information on the selected candidates
verbose("Candidate Tests:");
map { verbose("... $_") } @candidates;

###############################################################################
# If we're just doing a dry-run, STOP!
if ($dryrun) {
    exit;
}

###############################################################################
# Make sure that the test runs cleanly on its own.
status("Verifying test passes in clean environment");
if (run_tests()) {
    die "FAIL: test fails when run in clean environment.\n";
}

###############################################################################
# Run the tests to verify failure and to count up the number of failing tests.
status("Reproducing test failure");

my $initial_failures = run_tests(@candidates);
unless ($initial_failures) {
    die "Selected candidate tests didn't produce failure:\n",
        map {"\t$_\n"} @candidates;
}
verbose("... failures: $initial_failures");

###############################################################################
# Do a binary search through the tests, removing ones that don't contribute to
# the initial failure count.
my %bisect_candidates = map { $_=>1 } @candidates;
bisect_test_removal( [@candidates] );
my @shortest_set = grep { exists $bisect_candidates{$_} } @candidates;

###############################################################################
# Display the set of tests we're left with; that's the minimal set of tests
# needed to reproduce the failure.
status("Shortest set of tests needed to reproduce failure:");
status(map {"\t$_"} @shortest_set, $unittest);

###############################################################################
# All done; exit peacefully.
exit;


###############################################################################
# Bisect our way through the list of candidate tests, chucking out whole sets
# of tests that aren't contributing to the initial failure count.
#
# Optimized for...
# - breadth first traversal, so we can throw away larger chunks of tests as
#   early as possible
# - faster LHS/RHS selection; if LHS doesn't contribute to failures it *must*
#   be in the RHS (so don't run it now, just recurse and bisect further)
sub bisect_test_removal {
    my @chunks = @_;
    return unless @chunks;

    # Bisect each of the provided chunks of tests, breadth-first
    my @remaining;
    foreach my $chunk (@chunks) {
        # skip this chunk if it doesn't contain any viable test candidates
        next unless (any { exists $bisect_candidates{$_} } @{$chunk});

        # bisect the chunk
        my @rhs = @{$chunk};
        my @lhs = splice @rhs, 0, (scalar(@rhs) / 2);

        # run each side, throwing out tests that aren't contributing
      SIDE:
        foreach my $side (\@lhs, \@rhs) {
            # skip side if its empty
            next unless @{$side};

            # build list of tests to run, *ignoring* the ones on this side.
            # List is *always* built up directly from the original set of
            # candidate tests, trimming it down to just the ones we still care
            # about.
            my %ignore = map { $_ => 1 } @{$side};
            my @to_run =
                grep { !exists $ignore{$_} }
                grep {  exists $bisect_candidates{$_} }
                @candidates;
            verbose("... testing removal of:");
            verbose("... ... $_") for @{$side};

            # run the tests, throwing this side away if its not contributing
            my $failures = run_tests(@to_run);
            if ($failures == $initial_failures) {
                verbose("... ... failures unchanged; removing tests");
                delete $bisect_candidates{$_} for @{$side};

                # if the LHS didn't contribute, we *know* its something in the
                # RHS; add it to the list and move on without running it now.
                if ($side == \@lhs) {
                    push @remaining, \@rhs;
                    last SIDE;
                }
            }
            else {
                verbose("... ... failures changed ($failures)");
                push @remaining, $side if (@{$side} > 1);
            }
        }
    }

    # recurse, processing the remaining chunks breadth-first
    bisect_test_removal(@remaining);
}

###############################################################################
# Checks to see if the given fixture is used by the test.  Returns true if it
# is, false otherwise.
sub is_fixture_used_by_test {
    my ($fixture, $test) = @_;
    my %fixtures = map { $_ => 1 } fixtures_used_by_test($test);
    return defined $fixtures{$fixture} ? 1 : 0;
}

###############################################################################
# Returns a list of the fixtures that are used by the given unit test.
sub fixtures_used_by_test {
    my $test = shift;
    my %fixtures;

    my @lines = grep {/^\s*fixtures\(/} slurp $test;
    foreach my $line (@lines) {
        $line =~ s/fixtures\((.+)\)\s*;/$1/;              # remove fixtures()
        $line =~ s/qw[\(\[\{\|](.+)[\)\]\}\|]\s*$/$1/;    # remove qw()
        $line =~ s/,//g;                                  # remove commas
    }

    my @words = quotewords('\s+', 0, @lines);
    foreach my $word (grep { defined $_ } @words) {
        $fixtures{$word}++;
    }

    return keys %fixtures;
}

###############################################################################
# Runs the tests, and returns the number of failures.
sub run_tests {
    my @tests = @_;

    # clean out the test environment
    clean_test_environment();

    # run the tests
    my @results =
        grep {/Tests:.*Failed/} `prove -l -Q @tests $unittest 2>/dev/null`;

    # find the failures from the unit test that we're running.  If _other_
    # tests failed along the way we don't care.
    my $total_failures = 0;
    foreach my $line (@results) {
        if ($line =~ /^(\S+).*Tests:\s+\d+\s+Failed:\s+(\d+)/) {
            my ($short_testname, $failed) = ($1, $2);
            if ($count_all_failures) {
                $total_failures += $failed;
            }
            elsif ($unittest =~ /$short_testname/) {
                return $failed;
            }
        }
    }

    return $total_failures;
}

###############################################################################
# Cleans out the test environment
sub clean_test_environment {
    my $HOME = $ENV{HOME};
    rmtree("$HOME/.nlw", 0);
    rmtree("$HOME/stci/socialtext/nlw/t/tmp");
    rmtree("$HOME/src/st/socialtext/nlw/t/tmp");

    my $USERNAME = $ENV{USER};
    `dropdb NLW_${USERNAME}_testing 2>/dev/null`;
}

###############################################################################
# Spits out status.
sub status {
    map { print "$_\n" } @_;
}

sub verbose {
    status(@_) if ($verbose);
}


=head1 NAME

why-failing - find out why a unit test is failing

=head1 SYNOPSIS

  why-failing [options] <test>

  Options:
    --stci <url>        URL to STCI results (default Galena)
    --branch <branch>   Branch test is failing on (default 'master')
    --run <testrun>     Test run test is failing on (default 'current')
    --candidate <test>  Specify candidate tests explicitly
    --all               Count all failures, not just ones in test
    --dryrun            Dry-run; show candidates, but don't run tests
    --verbose           Display verbose output
    --help/-?           Display a brief usage statement
    --man               Display full man page

  Example:
    why-failing -v -b iteration-2009-03-27 t/Socialtext/HTMLArchive.t

=head1 DESCRIPTION

C<why-failing> aims to help narrow the focus as to why a unit test might be
failing.

Our test fixtures are cached from one test to the next, so its I<entirely
possible> that the test isn't failing because of something it is (or isn't)
doing, but because some other test stomped on the test environment and left a
mess around which is tripping up the failing test.

C<why-failing> tries to help narrow the focus on finding out why it is that
the test is failing, by pulling up the list of tests that ran before this one
and zeroing on the smallest set of tests necessary to reproduce the failure.

With that information in hand, you can then examine those tests in order to
determine what's going on and why they're causing failure.

B<NOTE>, when run, this script will B<forcably clean out your test
environment>, including:

=over

=item * dropping your test DB

=item * erasing ~/.nlw/

=item * erasing t/tmp/ in your dev-env

=item * erasing t/tmp/ in your stci environment

=back

=head1 OPTIONS

=over

=item B<--stci E<lt>urlE<gt>>

Specifies the URL to the STCI test results server.

Defaults to C<http://galena.socialtext.net/stci>

=item B<--branch E<lt>branchE<gt>>

Specifies the branch whose test results we should be examining.

Defaults to "master".

=item B<--run E<lt>testrunE<gt>>

Specifies the test run whose results we should be examining.

Defaults to "current", but "in-progress" is another viable option.

=item B<--candidate E<lt>testE<gt>>

Specifies the list of candidate tests.  If not specified, the list of
candidate tests is calculated automatically.

To specify multiple candidate tests, use this command line option multiple
times, in order.

=item B<--all>

When checking "did the test pass?", consider failures for I<all> of the tests
that were run, not just the one we're checking.

By default, we B<only> check for failures in the test we're looking at, and we
ignore failures in the candidates.  With C<--all>, we also track failures in
the candidates.

Useful for helping hunt down chains of failures in a single run.

=item B<--dryrun>

Dry-run; show the selected candidate tests, but don't actually do a test run.

Implies C<--verbose>.

=item B<--verbose>

Displays verbose output while zeroing in on shortest failure path.

=item B<--help>

Displays a brief usage statement.

=item B<--man>

Displays the man page.

=back

=head1 AUTHOR

Graham TerMarsch (graham.termarsch@socialtext.com)

=head1 COPYRIGHT

Copyright 2009 Socialtext, Inc., All Rights Reserved.

=cut
